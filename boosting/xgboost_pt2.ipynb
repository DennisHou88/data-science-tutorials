{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: advanced tips and hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is the second part of our series on XGBoost. If you haven't done it yet, for an introduction to XGBoost check [Getting started with XGBoost](https://cambridgespark.com/content/tutorials/getting-started-with-xgboost/index.html).\n",
    "\n",
    "With this tutorial you will learn to use the native XGBoost API (for the sklearn API see the previous tutorial) that comes with its own cross-validation and other nice features. You will learn the role of the main hyperpameters and techniques to tune your model.\n",
    "\n",
    "Other topics that you will come across in that tutorial include:\n",
    "- Tuning XGboost hyperparameters\n",
    "- Using a watchlist and early_stopping_round with XGBoost's native API\n",
    "- DMatrices (XGBoost data format)\n",
    "- Bias and variance trade off\n",
    "- Timing in a Jupyter notebook\n",
    "- Cross-Validation\n",
    "- Using a baseline model\n",
    "- Mean Absoluate Error\n",
    "- Grid Search\n",
    "- Saving and loading an XGboost model\n",
    "\n",
    "Let's start with a short introduction to the XGBoost native API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The native XGBoost API\n",
    "\n",
    "Although the scikit-learn API of XGBoost that we have used in the previous tutorial is easy to use and fits well in a scikit-learn pipeline, it is sometimes better to use the native API. Advantages include:\n",
    "- The possibility to automatically find the best number of boosting rounds\n",
    "- A built-in cross validation function\n",
    "- Possibility to use custom objective functions\n",
    "- Find more details [here](http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training)\n",
    "\n",
    "### DMatrices\n",
    "Instead of directly passing numpy arrays or pandas dataFrame to the algorithm, XGBoost require to first create a DMatrix, which is its own data format. A DMatrix can contain both the features and the target. If you already have loaded you data into numpy arrays X and y, you can create a DMatrix with:\n",
    "\n",
    "```xgb.DMatrix(X, label=y)```\n",
    "\n",
    "To read more about DMatrices check [the documentation](http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data/problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will solve a regression problem here, but what you will learn is also applicable to classification. Download the dataset [here](https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset) and unzip it.\n",
    "\n",
    "This dataset is composed of 53 features describing a given post on Facebook, such as the number of likes on the page it was posted, the category of the page, the time and day it was posted, etc ... The last column is the target, it is the number of comments the post received. Our goal is to train a model that can predict the number of comments a new post will receive based on all the above features.\n",
    "\n",
    "First make sure you install the libraries we will use for this tutorial. \n",
    "You need to install XGBoost (see previous tutorial), pandas and numpy. If you are using ```pip```, you can do it by executing the following command in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages (0.71)\r\n",
      "Requirement already satisfied: scikit-learn in /home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages (0.19.1)\r\n",
      "Requirement already satisfied: pandas in /home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages (0.22.0)\r\n",
      "Requirement already satisfied: numpy in /home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages (1.14.2)\r\n",
      "Requirement already satisfied: scipy in /home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages (from xgboost) (1.0.1)\r\n",
      "Requirement already satisfied: pytz>=2011k in /home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages (from pandas) (2018.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2 in /home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages (from pandas) (2.7.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages (from python-dateutil>=2->pandas) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40419</th>\n",
       "      <td>1433518</td>\n",
       "      <td>25951</td>\n",
       "      <td>101466</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>914.0</td>\n",
       "      <td>146.437037</td>\n",
       "      <td>103.0</td>\n",
       "      <td>162.507828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>413095</td>\n",
       "      <td>0</td>\n",
       "      <td>19301</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>36.851064</td>\n",
       "      <td>24.0</td>\n",
       "      <td>43.157198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21490</th>\n",
       "      <td>228007</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>12.323529</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.684425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18530</th>\n",
       "      <td>86582</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.785507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27887</th>\n",
       "      <td>3319</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.619048</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.906842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22340</th>\n",
       "      <td>663025</td>\n",
       "      <td>0</td>\n",
       "      <td>5954</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>58.287500</td>\n",
       "      <td>34.5</td>\n",
       "      <td>107.080833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20882</th>\n",
       "      <td>20329</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.129837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25312</th>\n",
       "      <td>367390</td>\n",
       "      <td>0</td>\n",
       "      <td>2678</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>11.753138</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.139026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15061</th>\n",
       "      <td>96918</td>\n",
       "      <td>100</td>\n",
       "      <td>4495</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>14.578313</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.760599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24241</th>\n",
       "      <td>2239009</td>\n",
       "      <td>51619</td>\n",
       "      <td>87722</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>92.891753</td>\n",
       "      <td>45.5</td>\n",
       "      <td>132.223076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1       2    3    4      5           6      7           8   \\\n",
       "40419  1433518  25951  101466    9  0.0  914.0  146.437037  103.0  162.507828   \n",
       "2968    413095      0   19301   24  0.0  331.0   36.851064   24.0   43.157198   \n",
       "21490   228007      0      24   16  0.0  304.0   12.323529    2.0   37.684425   \n",
       "18530    86582      0     175    4  0.0  124.0    6.333333    2.0   15.785507   \n",
       "27887     3319     10       1   17  0.0   30.0    3.619048    2.0    4.906842   \n",
       "22340   663025      0    5954   24  0.0  864.0   58.287500   34.5  107.080833   \n",
       "20882    20329      0     136   24  0.0   17.0    3.666667    2.0    4.129837   \n",
       "25312   367390      0    2678  100  0.0  107.0   11.753138    7.0   15.139026   \n",
       "15061    96918    100    4495    8  0.0  158.0   14.578313    8.0   24.760599   \n",
       "24241  2239009  51619   87722    9  0.0  688.0   92.891753   45.5  132.223076   \n",
       "\n",
       "        9  ...  44  45  46  47  48  49  50  51  52  53  \n",
       "40419  0.0 ...   0   0   0   0   0   0   0   0   1   1  \n",
       "2968   0.0 ...   0   0   0   0   0   0   0   0   1   0  \n",
       "21490  0.0 ...   0   1   0   0   1   0   0   0   0   1  \n",
       "18530  0.0 ...   0   0   0   0   1   0   0   0   0   0  \n",
       "27887  0.0 ...   0   1   0   0   1   0   0   0   0   0  \n",
       "22340  0.0 ...   0   0   0   0   0   0   0   1   0   4  \n",
       "20882  0.0 ...   0   0   0   0   1   0   0   0   0   1  \n",
       "25312  0.0 ...   0   0   0   0   0   0   0   1   0   1  \n",
       "15061  0.0 ...   0   0   0   0   0   0   0   0   1   0  \n",
       "24241  0.0 ...   0   0   0   0   0   0   1   0   0   0  \n",
       "\n",
       "[10 rows x 54 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/facebook_comments/Dataset/Training/Features_Variant_1.csv\", header=None)\n",
    "df.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 40949 lines and 54 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset contains {} lines and {} columns\".format(*df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of our model, we need to train it on a sample of the data and test it on an other. We can do this easily with the the function ```train_test_split``` from scikit-learn. First, let's extract the features and the target from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.loc[:,:52].values, df.loc[:,53].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep 90% of the dataset for training, and 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into DMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, in order to use the native API for XGBoost, we will first need to build DMatrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use mean absolute error to evaluate the quality of our predictions, which is a quite common and simple metric that has the advantage of being in the same unit as our target. You can read more about it [here](https://en.wikipedia.org/wiki/Mean_absolute_error). MAE is easy to compute, but scikit-learn provides a function that does it for us for free, so let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to get an idea of the range of MAE we would like to achieve here, we are going to build a baseline model, and save its score for later. This score is what we can achieve with no efforts, so we hope we will beat it with our fancy algorithms.\n",
    "\n",
    "For our baseline, we will keep things simple and predict that each new post will get the mean number of comments that we observed in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# \"Learn\" the mean from the training data\n",
    "mean_train = np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.278938514136865"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set\n",
    "baseline_predictions = np.ones(y_test.shape) * mean_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MAE\n",
    "mae_baseline = mean_absolute_error(y_test, baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE is 11.31\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline MAE is {:.2f}\".format(mae_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Tuning an XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick note on the method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we are going to see methods to tune the main parameters of your xgboost model. In an ideal world, with infinite ressources and where time is not an issue, you could run a giant grid search with all the parameters together and find the optimal solution. \n",
    "\n",
    "In fact, you might even be able to do that with really small datasets, but as the data grows bigger, training time grows too, and each step in the tuning process becomes more expensive. For this reason it is important to understand the role of the parameters and focus on the steps that we expect to impact our results the most. Here we will tune 6 of the hyperparameters that are usually having a big impact on performance. Whilst, again, it would be necessary to test all combinations to ensure we find *THE* optimal solution, our goal here is to find a good enough one by improving our out-of-the-box model with as few steps as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The params dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the parameters passed to XGBoost via the native API are defined in a dictionary. Let's define it with default values for the moment.\n",
    "You can find a list and a description of all parameters [here](http://xgboost.readthedocs.io/en/latest/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6, \n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3, \n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear', # We define a linear regression\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### num_boost_round (and early_stopping_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter we will look at is not part of the params dictionary, but will be passed as a standalone argument to the train method. \n",
    "This parameter is called \"num_boost_round\" and corresponds to the number of boosting rounds or trees to build. Its optimal value highly depends on the other parameters, and thus it should be re-tuned each time you update a parameter. You could do this by tuning it together with all parameters in a grid-search, but it can be quite time consuming. \n",
    "\n",
    "Fortunately XGBoost provides a nice way to find the best number of rounds whilst training. Since trees are built sequentially, instead of fixing the number of rounds at the beginning, we can test our model at each step and see if adding a new tree/round improves performance.\n",
    "\n",
    "To do so, we define a test dataset and a metric that is used to assess performance at each round. If performance haven't improved for N rounds (N is defined by the variable ```early_stopping_round```), we stop the training and keep the best number of boosting rounds. Let's see how to use it.\n",
    "\n",
    "First, we need to add the evaluation metric we are interested in to our params dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eval_metric'] = \"mae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to pass a num_boost_round which corresponds to the maximum number of boosting rounds that we allow.\n",
    "We set it to a large value hoping to find the optimal number of rounds before reaching it, if we haven't improved performance on our test dataset in ```early_stopping_round``` rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to automatically find the best number of boosting rounds, we need to pass extra parameters on top of the params dictionary, the training DMatrix and num_boost_round:\n",
    "- evals: a list of pairs (test_dmatrix, name_of_test). Here we will use our dtest DMatrix.\n",
    "- early_stopping_rounds: The number of rounds without improvements after which we should stop, here we set it to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-mae:5.97478\n",
      "Will train until Test-mae hasn't improved in 10 rounds.\n",
      "[1]\tTest-mae:5.03359\n",
      "[2]\tTest-mae:4.64572\n",
      "[3]\tTest-mae:4.42331\n",
      "[4]\tTest-mae:4.39328\n",
      "[5]\tTest-mae:4.35544\n",
      "[6]\tTest-mae:4.31315\n",
      "[7]\tTest-mae:4.33087\n",
      "[8]\tTest-mae:4.37164\n",
      "[9]\tTest-mae:4.38774\n",
      "[10]\tTest-mae:4.39443\n",
      "[11]\tTest-mae:4.40661\n",
      "[12]\tTest-mae:4.39124\n",
      "[13]\tTest-mae:4.39088\n",
      "[14]\tTest-mae:4.39827\n",
      "[15]\tTest-mae:4.39104\n",
      "[16]\tTest-mae:4.40307\n",
      "Stopping. Best iteration:\n",
      "[6]\tTest-mae:4.31315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(dtest, \"Test\")], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained the best MAE with 7 boosting rounds. Best MAE: 4.31\n"
     ]
    }
   ],
   "source": [
    "print(\"We obtained the best MAE with {} boosting rounds. Best MAE: {:.2f}\".format(model.best_iteration+1, model.best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we stopped before reaching the maximum number of boosting rounds, that's because after the 7th tree, adding more rounds did not lead to improvements of MAE on the test dataset. \n",
    "\n",
    "Let's keep this MAE in mind for later, this is the MAE of our model with default parameters and an optimal number of boosting rounds, on the test dataset. As you can see, we are already beating the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using xgboost's CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tune the other hyperparameters, we will use the cv function from xgboost. It allows us to run cross-validation on our training dataset and returns a mean MAE score.\n",
    "\n",
    "We need to pass it:\n",
    "- params: our dictionary of parameters.\n",
    "- our dtrain matrix.\n",
    "- num_boost_round: number of boosting rounds. Here we will use a large number again and count on early_stopping_rounds to find the optimal number of rounds before reaching the maximum.\n",
    "- seed: random seed. It's important to set a seed here, to ensure we are using the same folds for each step so we can properly compare the scores with different parameters.\n",
    "- nfold: the number of folds to use for cross-validation\n",
    "- metrics: the metrics to use to evaluate our model, here we use MAE.\n",
    "\n",
    "As you can see, we don't need to pass a test dataset here. It's because the cross-validation function is splitting the train dataset into nfolds and iteratively keeps one of the folds for test purposes. You can read more about it <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what cross-validation score we get with our current parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = xgb.cv(params, dtrain, num_boost_round=num_boost_round, seed=42, nfold=5, metrics={'mae'}, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-mae-mean</th>\n",
       "      <th>test-mae-std</th>\n",
       "      <th>train-mae-mean</th>\n",
       "      <th>train-mae-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.689189</td>\n",
       "      <td>0.270149</td>\n",
       "      <td>5.604765</td>\n",
       "      <td>0.064495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.849525</td>\n",
       "      <td>0.271883</td>\n",
       "      <td>4.622477</td>\n",
       "      <td>0.065106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.468342</td>\n",
       "      <td>0.239475</td>\n",
       "      <td>4.059710</td>\n",
       "      <td>0.065772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.268584</td>\n",
       "      <td>0.224462</td>\n",
       "      <td>3.722983</td>\n",
       "      <td>0.060820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.192448</td>\n",
       "      <td>0.189762</td>\n",
       "      <td>3.510303</td>\n",
       "      <td>0.061203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.172856</td>\n",
       "      <td>0.189612</td>\n",
       "      <td>3.367213</td>\n",
       "      <td>0.061021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.157860</td>\n",
       "      <td>0.192572</td>\n",
       "      <td>3.245549</td>\n",
       "      <td>0.060276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.143254</td>\n",
       "      <td>0.194440</td>\n",
       "      <td>3.151495</td>\n",
       "      <td>0.062612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.147843</td>\n",
       "      <td>0.196197</td>\n",
       "      <td>3.082321</td>\n",
       "      <td>0.059020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.144657</td>\n",
       "      <td>0.189785</td>\n",
       "      <td>3.016803</td>\n",
       "      <td>0.057321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.151564</td>\n",
       "      <td>0.184621</td>\n",
       "      <td>2.974848</td>\n",
       "      <td>0.048560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.153975</td>\n",
       "      <td>0.192428</td>\n",
       "      <td>2.929269</td>\n",
       "      <td>0.034235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.154961</td>\n",
       "      <td>0.192741</td>\n",
       "      <td>2.900030</td>\n",
       "      <td>0.037107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.150087</td>\n",
       "      <td>0.185076</td>\n",
       "      <td>2.870198</td>\n",
       "      <td>0.039780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.156701</td>\n",
       "      <td>0.188260</td>\n",
       "      <td>2.846479</td>\n",
       "      <td>0.037792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.145404</td>\n",
       "      <td>0.184212</td>\n",
       "      <td>2.811917</td>\n",
       "      <td>0.036969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.141670</td>\n",
       "      <td>0.183382</td>\n",
       "      <td>2.791487</td>\n",
       "      <td>0.034505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.139406</td>\n",
       "      <td>0.191476</td>\n",
       "      <td>2.764857</td>\n",
       "      <td>0.032798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.134138</td>\n",
       "      <td>0.196838</td>\n",
       "      <td>2.730498</td>\n",
       "      <td>0.030304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.120616</td>\n",
       "      <td>0.194074</td>\n",
       "      <td>2.686291</td>\n",
       "      <td>0.025828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.115625</td>\n",
       "      <td>0.190053</td>\n",
       "      <td>2.661713</td>\n",
       "      <td>0.024625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.111492</td>\n",
       "      <td>0.195630</td>\n",
       "      <td>2.632685</td>\n",
       "      <td>0.026922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.106884</td>\n",
       "      <td>0.188216</td>\n",
       "      <td>2.598391</td>\n",
       "      <td>0.026591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.109425</td>\n",
       "      <td>0.189713</td>\n",
       "      <td>2.575809</td>\n",
       "      <td>0.034930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.106895</td>\n",
       "      <td>0.185098</td>\n",
       "      <td>2.544730</td>\n",
       "      <td>0.030703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.103488</td>\n",
       "      <td>0.185234</td>\n",
       "      <td>2.525501</td>\n",
       "      <td>0.031801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.098627</td>\n",
       "      <td>0.186998</td>\n",
       "      <td>2.500908</td>\n",
       "      <td>0.027606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.098459</td>\n",
       "      <td>0.185767</td>\n",
       "      <td>2.478799</td>\n",
       "      <td>0.029692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.098721</td>\n",
       "      <td>0.186931</td>\n",
       "      <td>2.457476</td>\n",
       "      <td>0.027944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.093534</td>\n",
       "      <td>0.187031</td>\n",
       "      <td>2.433839</td>\n",
       "      <td>0.019612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.092398</td>\n",
       "      <td>0.190349</td>\n",
       "      <td>2.413498</td>\n",
       "      <td>0.019445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.092814</td>\n",
       "      <td>0.189468</td>\n",
       "      <td>2.398102</td>\n",
       "      <td>0.020367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.094014</td>\n",
       "      <td>0.191986</td>\n",
       "      <td>2.373646</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.090828</td>\n",
       "      <td>0.183624</td>\n",
       "      <td>2.354041</td>\n",
       "      <td>0.021126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.091551</td>\n",
       "      <td>0.183067</td>\n",
       "      <td>2.341457</td>\n",
       "      <td>0.014121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.092410</td>\n",
       "      <td>0.179464</td>\n",
       "      <td>2.325629</td>\n",
       "      <td>0.011132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.086722</td>\n",
       "      <td>0.183275</td>\n",
       "      <td>2.307251</td>\n",
       "      <td>0.014028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.082788</td>\n",
       "      <td>0.184458</td>\n",
       "      <td>2.292440</td>\n",
       "      <td>0.011218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
       "0        5.689189      0.270149        5.604765       0.064495\n",
       "1        4.849525      0.271883        4.622477       0.065106\n",
       "2        4.468342      0.239475        4.059710       0.065772\n",
       "3        4.268584      0.224462        3.722983       0.060820\n",
       "4        4.192448      0.189762        3.510303       0.061203\n",
       "5        4.172856      0.189612        3.367213       0.061021\n",
       "6        4.157860      0.192572        3.245549       0.060276\n",
       "7        4.143254      0.194440        3.151495       0.062612\n",
       "8        4.147843      0.196197        3.082321       0.059020\n",
       "9        4.144657      0.189785        3.016803       0.057321\n",
       "10       4.151564      0.184621        2.974848       0.048560\n",
       "11       4.153975      0.192428        2.929269       0.034235\n",
       "12       4.154961      0.192741        2.900030       0.037107\n",
       "13       4.150087      0.185076        2.870198       0.039780\n",
       "14       4.156701      0.188260        2.846479       0.037792\n",
       "15       4.145404      0.184212        2.811917       0.036969\n",
       "16       4.141670      0.183382        2.791487       0.034505\n",
       "17       4.139406      0.191476        2.764857       0.032798\n",
       "18       4.134138      0.196838        2.730498       0.030304\n",
       "19       4.120616      0.194074        2.686291       0.025828\n",
       "20       4.115625      0.190053        2.661713       0.024625\n",
       "21       4.111492      0.195630        2.632685       0.026922\n",
       "22       4.106884      0.188216        2.598391       0.026591\n",
       "23       4.109425      0.189713        2.575809       0.034930\n",
       "24       4.106895      0.185098        2.544730       0.030703\n",
       "25       4.103488      0.185234        2.525501       0.031801\n",
       "26       4.098627      0.186998        2.500908       0.027606\n",
       "27       4.098459      0.185767        2.478799       0.029692\n",
       "28       4.098721      0.186931        2.457476       0.027944\n",
       "29       4.093534      0.187031        2.433839       0.019612\n",
       "30       4.092398      0.190349        2.413498       0.019445\n",
       "31       4.092814      0.189468        2.398102       0.020367\n",
       "32       4.094014      0.191986        2.373646       0.023400\n",
       "33       4.090828      0.183624        2.354041       0.021126\n",
       "34       4.091551      0.183067        2.341457       0.014121\n",
       "35       4.092410      0.179464        2.325629       0.011132\n",
       "36       4.086722      0.183275        2.307251       0.014028\n",
       "37       4.082788      0.184458        2.292440       0.011218"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cv``` returns a table where the rows correspond to the number of boosting trees used, here again, we stopped before the 999 rounds (fortunately!).\n",
    "\n",
    "The 4 columns correspond to the mean and standard deviation of MAE on the test dataset and on the train dataset. For this tutorial we will only try to improve the mean test MAE. We can get the best MAE score from ```cv``` with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0827876000000005"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test-mae-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to use ```cv```, we are ready to start tuning! We will first tune our parameters to minimize the MAE on cross-validation, and then check the performance of our model on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those parameters add constraints on the architecture of the trees. \n",
    "\n",
    "- max_depth is the maximum number of nodes allowed from the root the farthest leaf of a tree. Deeper trees can model more complex relationship but are more likely to overfit.\n",
    "- min_child_weight is the minimum weight (or number of samples if all samples have a weight of 1) required in order to create a new node in the tree. A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit.\n",
    "\n",
    "Thus, those parameters can be used to control the complexity of the trees. It is important to tune them together in order to find a good [trade-off between model bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a list containing all the combinations max_depth/min_child_weight that we want to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try wider intervals with a larger step between each value and then narrow it down. \n",
    "# Here after several iteration I found that the optimal value was in the following ranges.\n",
    "gridsearch_params = [(max_depth, min_child_weight) for max_depth in range(9,12) for min_child_weight in range(5,8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run cross validation on each of those pairs. It can take some time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run CV with max_depth=9, min_child_weight=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.04524 for 6 boosting rounds (STD 0.18642644398582528))\n",
      "Run CV with max_depth=9, min_child_weight=6\n",
      "\tMAE 4.0764622 for 5 boosting rounds (STD 0.25751775618345235))\n",
      "Run CV with max_depth=9, min_child_weight=7\n",
      "\tMAE 4.0753928 for 5 boosting rounds (STD 0.195456330433578))\n",
      "Run CV with max_depth=10, min_child_weight=5\n",
      "\tMAE 4.0805826000000005 for 5 boosting rounds (STD 0.2023071500452715))\n",
      "Run CV with max_depth=10, min_child_weight=6\n",
      "\tMAE 4.035100600000001 for 5 boosting rounds (STD 0.29236995095023033))\n",
      "Run CV with max_depth=10, min_child_weight=7\n",
      "\tMAE 4.0872416000000005 for 5 boosting rounds (STD 0.18076034617094552))\n",
      "Run CV with max_depth=11, min_child_weight=5\n",
      "\tMAE 4.062633 for 5 boosting rounds (STD 0.20638935284553792))\n",
      "Run CV with max_depth=11, min_child_weight=6\n",
      "\tMAE 4.054831999999999 for 5 boosting rounds (STD 0.2739467528991721))\n",
      "Run CV with max_depth=11, min_child_weight=7\n",
      "\tMAE 4.0581036 for 5 boosting rounds (STD 0.19519241685946723))\n",
      "Best params: 10, 6, MAE: 4.035100600000001\n"
     ]
    }
   ],
   "source": [
    "# Define initial best params and MAE\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"Run CV with max_depth={}, min_child_weight={}\".format(max_depth, min_child_weight))\n",
    "    \n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    \n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(params, dtrain, num_boost_round=num_boost_round, seed=42, nfold=5, metrics={'mae'}, early_stopping_rounds=10)\n",
    "    \n",
    "    # Update best MAE\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    std = cv_results['test-mae-std'][boost_rounds]\n",
    "    print(\"\\tMAE {} for {} boosting rounds (STD {}))\".format(mean_mae, boost_rounds, std))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "        \n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the best score with a depth of 10 and min_child_weight of 6, so let's update our params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = 10\n",
    "params['min_child_weight'] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those parameters control the sampling of the dataset that is done at each boosting round. \n",
    "\n",
    "Instead of using the whole training set everytime, we can build a tree on slightly different data at each step, which makes it less likely to overfit to a single sample or feature. \n",
    "- subsample corresponds to the fraction of observations (the rows) to subsample at each step. By default it is set to 1 meaning that we use all rows.\n",
    "- colsample_bytree corresponds to the fraction of features (the columns) to use. By default it is set to 1 meaning that we will use all features.\n",
    "\n",
    "Let's see if we can get better results by tuning those parameters together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [(subsample, colsample) for subsample in [i/10. for i in range(7,11)] for colsample in [i/10. for i in range(7,11)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run CV with subsample=1.0, colsample=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages/ipykernel_launcher.py:18: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.035100600000001 for 5 boosting rounds (STD 0.29236995095023033))\n",
      "Run CV with subsample=1.0, colsample=0.9\n",
      "\tMAE 4.078552 for 6 boosting rounds (STD 0.22851713233191084))\n",
      "Run CV with subsample=1.0, colsample=0.8\n",
      "\tMAE 4.1998722 for 5 boosting rounds (STD 0.24177582405393647))\n",
      "Run CV with subsample=1.0, colsample=0.7\n",
      "\tMAE 4.2189922 for 6 boosting rounds (STD 0.23415296799519736))\n",
      "Run CV with subsample=0.9, colsample=1.0\n",
      "\tMAE 4.0413458 for 5 boosting rounds (STD 0.26891675920284325))\n",
      "Run CV with subsample=0.9, colsample=0.9\n",
      "\tMAE 4.046795 for 5 boosting rounds (STD 0.22136935968015076))\n",
      "Run CV with subsample=0.9, colsample=0.8\n",
      "\tMAE 4.076019 for 5 boosting rounds (STD 0.27720143212400616))\n",
      "Run CV with subsample=0.9, colsample=0.7\n",
      "\tMAE 4.1564212 for 6 boosting rounds (STD 0.19764143777194093))\n",
      "Run CV with subsample=0.8, colsample=1.0\n",
      "\tMAE 4.084800400000001 for 5 boosting rounds (STD 0.20935706392438744))\n",
      "Run CV with subsample=0.8, colsample=0.9\n",
      "\tMAE 4.0249596 for 6 boosting rounds (STD 0.1923843861290202))\n",
      "Run CV with subsample=0.8, colsample=0.8\n",
      "\tMAE 4.1415168 for 6 boosting rounds (STD 0.23582096934954713))\n",
      "Run CV with subsample=0.8, colsample=0.7\n",
      "\tMAE 4.1739048 for 6 boosting rounds (STD 0.1557401275271085))\n",
      "Run CV with subsample=0.7, colsample=1.0\n",
      "\tMAE 4.1091618 for 5 boosting rounds (STD 0.21539740846853292))\n",
      "Run CV with subsample=0.7, colsample=0.9\n",
      "\tMAE 4.111978800000001 for 5 boosting rounds (STD 0.205012375220034))\n",
      "Run CV with subsample=0.7, colsample=0.8\n",
      "\tMAE 4.1505806 for 5 boosting rounds (STD 0.26327372320351305))\n",
      "Run CV with subsample=0.7, colsample=0.7\n",
      "\tMAE 4.1847912 for 6 boosting rounds (STD 0.2104498727054973))\n",
      "Best params: 0.8, 0.9, MAE: 4.0249596\n"
     ]
    }
   ],
   "source": [
    "# This can take some time ...\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"Run CV with subsample={}, colsample={}\".format(subsample, colsample))\n",
    "    \n",
    "    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    \n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(params, dtrain, num_boost_round=num_boost_round, seed=42, nfold=5, metrics={'mae'}, early_stopping_rounds=10)\n",
    "    \n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    std = cv_results['test-mae-std'][boost_rounds]\n",
    "    print(\"\\tMAE {} for {} boosting rounds (STD {}))\".format(mean_mae, boost_rounds, std))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (subsample,colsample)\n",
    "        \n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we update our params dictionary\n",
    "params['subsample'] = .8\n",
    "params['colsample_bytree'] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ETA parameter controls the learning rate. It corresponds to the shrinkage of the weights associated to features after each round, in other words it defines the amount of \"correction\" we make at each step (remember how each boosting round is correcting the errors of the previous? if not, check our first tutorial [here](https://cambridgespark.com/content/tutorials/getting-started-with-xgboost/index.html)). \n",
    "\n",
    "In practice, having a lower eta makes our model more robust to overfiting thus, usually, the lower the learning rate, the best. But with a lower eta, we need more boosting rounds, which takes more time to train, sometimes for only marginal improvements. Let's try a couple of values here, and time them with the notebook command:\n",
    "\n",
    "```%time```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run CV with eta=0.3\n",
      "CPU times: user 18 s, sys: 48.2 ms, total: 18.1 s\n",
      "Wall time: 4.96 s\n",
      "\tMAE 4.084800400000001 for 5 boosting rounds\n",
      "\n",
      "Run CV with eta=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 s, sys: 87.5 ms, total: 24 s\n",
      "Wall time: 6.71 s\n",
      "\tMAE 3.9759386 for 9 boosting rounds\n",
      "\n",
      "Run CV with eta=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 s, sys: 111 ms, total: 36.6 s\n",
      "Wall time: 10.3 s\n",
      "\tMAE 3.9134848000000004 for 18 boosting rounds\n",
      "\n",
      "Run CV with eta=0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 124 ms, total: 1min 4s\n",
      "Wall time: 17.8 s\n",
      "\tMAE 3.8523240000000003 for 42 boosting rounds\n",
      "\n",
      "Run CV with eta=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 16s, sys: 359 ms, total: 4min 17s\n",
      "Wall time: 1min 7s\n",
      "\tMAE 3.8292826 for 221 boosting rounds\n",
      "\n",
      "Run CV with eta=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 55s, sys: 687 ms, total: 8min 55s\n",
      "Wall time: 2min 22s\n",
      "\tMAE 3.8301146000000004 for 460 boosting rounds\n",
      "\n",
      "Best params: 0.01, MAE: 3.8292826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/projects/cspark/teaching/data-science-tutorials/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# This can take some time ...\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"Run CV with eta={}\".format(eta))\n",
    "    \n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "    \n",
    "    # Run and time CV\n",
    "    %time cv_results = xgb.cv(params, dtrain, num_boost_round=num_boost_round, seed=42, nfold=5, metrics=['mae'], early_stopping_rounds=10)\n",
    "    \n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} boosting rounds\\n\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = eta\n",
    "        \n",
    "print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with the 2 last steps, by reducing eta from .01 to .005 we saved only ~.009 in MAE but went from 44s to 1min30s .. It looks like we start converging and our MAE is not getting much better. Depending on your goal, you might want to take the extra time for the little improvement in MAE, but here we'll stick to .01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eta'] = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how our final dictionary of parameters looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10,\n",
       " 'min_child_weight': 6,\n",
       " 'eta': 0.01,\n",
       " 'subsample': 0.8,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'objective': 'reg:linear',\n",
       " 'eval_metric': 'mae'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model with it and see how well it does on our test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Num boost round is still set to a large number\n",
    "num_boost_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-mae:7.69075\n",
      "Will train until Test-mae hasn't improved in 10 rounds.\n",
      "[1]\tTest-mae:7.62471\n",
      "[2]\tTest-mae:7.55557\n",
      "[3]\tTest-mae:7.48627\n",
      "[4]\tTest-mae:7.41617\n",
      "[5]\tTest-mae:7.34921\n",
      "[6]\tTest-mae:7.28237\n",
      "[7]\tTest-mae:7.21666\n",
      "[8]\tTest-mae:7.15639\n",
      "[9]\tTest-mae:7.09665\n",
      "[10]\tTest-mae:7.03385\n",
      "[11]\tTest-mae:6.97449\n",
      "[12]\tTest-mae:6.91345\n",
      "[13]\tTest-mae:6.85343\n",
      "[14]\tTest-mae:6.79441\n",
      "[15]\tTest-mae:6.73855\n",
      "[16]\tTest-mae:6.68239\n",
      "[17]\tTest-mae:6.62699\n",
      "[18]\tTest-mae:6.57245\n",
      "[19]\tTest-mae:6.52232\n",
      "[20]\tTest-mae:6.47018\n",
      "[21]\tTest-mae:6.42148\n",
      "[22]\tTest-mae:6.37245\n",
      "[23]\tTest-mae:6.32337\n",
      "[24]\tTest-mae:6.27463\n",
      "[25]\tTest-mae:6.22688\n",
      "[26]\tTest-mae:6.18184\n",
      "[27]\tTest-mae:6.13497\n",
      "[28]\tTest-mae:6.09259\n",
      "[29]\tTest-mae:6.04861\n",
      "[30]\tTest-mae:6.00289\n",
      "[31]\tTest-mae:5.95901\n",
      "[32]\tTest-mae:5.91646\n",
      "[33]\tTest-mae:5.87438\n",
      "[34]\tTest-mae:5.83302\n",
      "[35]\tTest-mae:5.79383\n",
      "[36]\tTest-mae:5.75745\n",
      "[37]\tTest-mae:5.71801\n",
      "[38]\tTest-mae:5.67694\n",
      "[39]\tTest-mae:5.63784\n",
      "[40]\tTest-mae:5.60201\n",
      "[41]\tTest-mae:5.56632\n",
      "[42]\tTest-mae:5.52869\n",
      "[43]\tTest-mae:5.49516\n",
      "[44]\tTest-mae:5.46097\n",
      "[45]\tTest-mae:5.42473\n",
      "[46]\tTest-mae:5.39473\n",
      "[47]\tTest-mae:5.36366\n",
      "[48]\tTest-mae:5.32982\n",
      "[49]\tTest-mae:5.29895\n",
      "[50]\tTest-mae:5.26875\n",
      "[51]\tTest-mae:5.2421\n",
      "[52]\tTest-mae:5.21166\n",
      "[53]\tTest-mae:5.18093\n",
      "[54]\tTest-mae:5.15061\n",
      "[55]\tTest-mae:5.12204\n",
      "[56]\tTest-mae:5.09411\n",
      "[57]\tTest-mae:5.06539\n",
      "[58]\tTest-mae:5.0375\n",
      "[59]\tTest-mae:5.0104\n",
      "[60]\tTest-mae:4.9843\n",
      "[61]\tTest-mae:4.96016\n",
      "[62]\tTest-mae:4.93316\n",
      "[63]\tTest-mae:4.90694\n",
      "[64]\tTest-mae:4.8803\n",
      "[65]\tTest-mae:4.85281\n",
      "[66]\tTest-mae:4.82797\n",
      "[67]\tTest-mae:4.80409\n",
      "[68]\tTest-mae:4.78115\n",
      "[69]\tTest-mae:4.75852\n",
      "[70]\tTest-mae:4.7384\n",
      "[71]\tTest-mae:4.71667\n",
      "[72]\tTest-mae:4.69386\n",
      "[73]\tTest-mae:4.67284\n",
      "[74]\tTest-mae:4.65211\n",
      "[75]\tTest-mae:4.63167\n",
      "[76]\tTest-mae:4.61204\n",
      "[77]\tTest-mae:4.5945\n",
      "[78]\tTest-mae:4.57809\n",
      "[79]\tTest-mae:4.56094\n",
      "[80]\tTest-mae:4.54353\n",
      "[81]\tTest-mae:4.52555\n",
      "[82]\tTest-mae:4.50861\n",
      "[83]\tTest-mae:4.49472\n",
      "[84]\tTest-mae:4.47965\n",
      "[85]\tTest-mae:4.46381\n",
      "[86]\tTest-mae:4.4481\n",
      "[87]\tTest-mae:4.43466\n",
      "[88]\tTest-mae:4.41948\n",
      "[89]\tTest-mae:4.4054\n",
      "[90]\tTest-mae:4.3902\n",
      "[91]\tTest-mae:4.3762\n",
      "[92]\tTest-mae:4.3609\n",
      "[93]\tTest-mae:4.34803\n",
      "[94]\tTest-mae:4.33517\n",
      "[95]\tTest-mae:4.32287\n",
      "[96]\tTest-mae:4.30998\n",
      "[97]\tTest-mae:4.29522\n",
      "[98]\tTest-mae:4.28121\n",
      "[99]\tTest-mae:4.26875\n",
      "[100]\tTest-mae:4.25473\n",
      "[101]\tTest-mae:4.24275\n",
      "[102]\tTest-mae:4.23348\n",
      "[103]\tTest-mae:4.22276\n",
      "[104]\tTest-mae:4.21309\n",
      "[105]\tTest-mae:4.202\n",
      "[106]\tTest-mae:4.19206\n",
      "[107]\tTest-mae:4.18078\n",
      "[108]\tTest-mae:4.17019\n",
      "[109]\tTest-mae:4.16014\n",
      "[110]\tTest-mae:4.15023\n",
      "[111]\tTest-mae:4.14091\n",
      "[112]\tTest-mae:4.13317\n",
      "[113]\tTest-mae:4.12463\n",
      "[114]\tTest-mae:4.11366\n",
      "[115]\tTest-mae:4.10475\n",
      "[116]\tTest-mae:4.09694\n",
      "[117]\tTest-mae:4.09148\n",
      "[118]\tTest-mae:4.0843\n",
      "[119]\tTest-mae:4.07714\n",
      "[120]\tTest-mae:4.06831\n",
      "[121]\tTest-mae:4.06226\n",
      "[122]\tTest-mae:4.05606\n",
      "[123]\tTest-mae:4.05336\n",
      "[124]\tTest-mae:4.05076\n",
      "[125]\tTest-mae:4.04404\n",
      "[126]\tTest-mae:4.03707\n",
      "[127]\tTest-mae:4.03017\n",
      "[128]\tTest-mae:4.02547\n",
      "[129]\tTest-mae:4.02114\n",
      "[130]\tTest-mae:4.01823\n",
      "[131]\tTest-mae:4.01236\n",
      "[132]\tTest-mae:4.00853\n",
      "[133]\tTest-mae:4.00328\n",
      "[134]\tTest-mae:4.00031\n",
      "[135]\tTest-mae:3.99619\n",
      "[136]\tTest-mae:3.9911\n",
      "[137]\tTest-mae:3.99018\n",
      "[138]\tTest-mae:3.98681\n",
      "[139]\tTest-mae:3.98236\n",
      "[140]\tTest-mae:3.98202\n",
      "[141]\tTest-mae:3.97716\n",
      "[142]\tTest-mae:3.97403\n",
      "[143]\tTest-mae:3.97136\n",
      "[144]\tTest-mae:3.96679\n",
      "[145]\tTest-mae:3.96538\n",
      "[146]\tTest-mae:3.9636\n",
      "[147]\tTest-mae:3.96067\n",
      "[148]\tTest-mae:3.95851\n",
      "[149]\tTest-mae:3.95536\n",
      "[150]\tTest-mae:3.95254\n",
      "[151]\tTest-mae:3.95208\n",
      "[152]\tTest-mae:3.9525\n",
      "[153]\tTest-mae:3.95033\n",
      "[154]\tTest-mae:3.94963\n",
      "[155]\tTest-mae:3.94717\n",
      "[156]\tTest-mae:3.94382\n",
      "[157]\tTest-mae:3.94166\n",
      "[158]\tTest-mae:3.93969\n",
      "[159]\tTest-mae:3.93756\n",
      "[160]\tTest-mae:3.93566\n",
      "[161]\tTest-mae:3.93433\n",
      "[162]\tTest-mae:3.9305\n",
      "[163]\tTest-mae:3.92939\n",
      "[164]\tTest-mae:3.92822\n",
      "[165]\tTest-mae:3.92532\n",
      "[166]\tTest-mae:3.92422\n",
      "[167]\tTest-mae:3.92377\n",
      "[168]\tTest-mae:3.92337\n",
      "[169]\tTest-mae:3.92273\n",
      "[170]\tTest-mae:3.92114\n",
      "[171]\tTest-mae:3.92105\n",
      "[172]\tTest-mae:3.91842\n",
      "[173]\tTest-mae:3.91753\n",
      "[174]\tTest-mae:3.91874\n",
      "[175]\tTest-mae:3.91951\n",
      "[176]\tTest-mae:3.91841\n",
      "[177]\tTest-mae:3.91671\n",
      "[178]\tTest-mae:3.9173\n",
      "[179]\tTest-mae:3.91668\n",
      "[180]\tTest-mae:3.9161\n",
      "[181]\tTest-mae:3.91454\n",
      "[182]\tTest-mae:3.91396\n",
      "[183]\tTest-mae:3.91227\n",
      "[184]\tTest-mae:3.91142\n",
      "[185]\tTest-mae:3.91195\n",
      "[186]\tTest-mae:3.91211\n",
      "[187]\tTest-mae:3.9117\n",
      "[188]\tTest-mae:3.91089\n",
      "[189]\tTest-mae:3.90822\n",
      "[190]\tTest-mae:3.90832\n",
      "[191]\tTest-mae:3.90763\n",
      "[192]\tTest-mae:3.90793\n",
      "[193]\tTest-mae:3.90941\n",
      "[194]\tTest-mae:3.91005\n",
      "[195]\tTest-mae:3.90899\n",
      "[196]\tTest-mae:3.91035\n",
      "[197]\tTest-mae:3.90799\n",
      "[198]\tTest-mae:3.90611\n",
      "[199]\tTest-mae:3.90654\n",
      "[200]\tTest-mae:3.90739\n",
      "[201]\tTest-mae:3.90839\n",
      "[202]\tTest-mae:3.90891\n",
      "[203]\tTest-mae:3.90854\n",
      "[204]\tTest-mae:3.90804\n",
      "[205]\tTest-mae:3.90829\n",
      "[206]\tTest-mae:3.90811\n",
      "[207]\tTest-mae:3.90785\n",
      "[208]\tTest-mae:3.90779\n",
      "Stopping. Best iteration:\n",
      "[198]\tTest-mae:3.90611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(dtest, \"Test\")], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained the best MAE with 199 boosting rounds. Best MAE: 3.91\n"
     ]
    }
   ],
   "source": [
    "print(\"We obtained the best MAE with {} boosting rounds. Best MAE: {:.2f}\".format(model.best_iteration+1, model.best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected it took us more rounds to get there, but we improved our MAE from 4.31 to 3.90, not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* step1 (num_rounds): 4.11\n",
    "* step2 (depth): 4.04\n",
    "* step3 (sampling): 4.02\n",
    "* step4 (eta): 3.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9082c53f60>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame([[\"Step 1 - Number of rounds\", 4.11], [\"Step 1 - Depth and Min child weight\", 4.04], \n",
    "              [\"Step 2 - Subsample and Colsample\", 4.02], [\"Step 4 - ETA\", 3.93]]).plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we found the best number of rounds, our model has been trained with more rounds than optimal, thus before using it for predictions, we should retrain it with the good number of rounds. Since we now the exact best num_boost_round, we don't need the early_stopping_round anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = model.best_iteration + 1 # index starts at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-mae:7.69075\n",
      "[1]\tTest-mae:7.62471\n",
      "[2]\tTest-mae:7.55557\n",
      "[3]\tTest-mae:7.48627\n",
      "[4]\tTest-mae:7.41617\n",
      "[5]\tTest-mae:7.34921\n",
      "[6]\tTest-mae:7.28237\n",
      "[7]\tTest-mae:7.21666\n",
      "[8]\tTest-mae:7.15639\n",
      "[9]\tTest-mae:7.09665\n",
      "[10]\tTest-mae:7.03385\n",
      "[11]\tTest-mae:6.97449\n",
      "[12]\tTest-mae:6.91345\n",
      "[13]\tTest-mae:6.85343\n",
      "[14]\tTest-mae:6.79441\n",
      "[15]\tTest-mae:6.73855\n",
      "[16]\tTest-mae:6.68239\n",
      "[17]\tTest-mae:6.62699\n",
      "[18]\tTest-mae:6.57245\n",
      "[19]\tTest-mae:6.52232\n",
      "[20]\tTest-mae:6.47018\n",
      "[21]\tTest-mae:6.42148\n",
      "[22]\tTest-mae:6.37245\n",
      "[23]\tTest-mae:6.32337\n",
      "[24]\tTest-mae:6.27463\n",
      "[25]\tTest-mae:6.22688\n",
      "[26]\tTest-mae:6.18184\n",
      "[27]\tTest-mae:6.13497\n",
      "[28]\tTest-mae:6.09259\n",
      "[29]\tTest-mae:6.04861\n",
      "[30]\tTest-mae:6.00289\n",
      "[31]\tTest-mae:5.95901\n",
      "[32]\tTest-mae:5.91646\n",
      "[33]\tTest-mae:5.87438\n",
      "[34]\tTest-mae:5.83302\n",
      "[35]\tTest-mae:5.79383\n",
      "[36]\tTest-mae:5.75745\n",
      "[37]\tTest-mae:5.71801\n",
      "[38]\tTest-mae:5.67694\n",
      "[39]\tTest-mae:5.63784\n",
      "[40]\tTest-mae:5.60201\n",
      "[41]\tTest-mae:5.56632\n",
      "[42]\tTest-mae:5.52869\n",
      "[43]\tTest-mae:5.49516\n",
      "[44]\tTest-mae:5.46097\n",
      "[45]\tTest-mae:5.42473\n",
      "[46]\tTest-mae:5.39473\n",
      "[47]\tTest-mae:5.36366\n",
      "[48]\tTest-mae:5.32982\n",
      "[49]\tTest-mae:5.29895\n",
      "[50]\tTest-mae:5.26875\n",
      "[51]\tTest-mae:5.2421\n",
      "[52]\tTest-mae:5.21166\n",
      "[53]\tTest-mae:5.18093\n",
      "[54]\tTest-mae:5.15061\n",
      "[55]\tTest-mae:5.12204\n",
      "[56]\tTest-mae:5.09411\n",
      "[57]\tTest-mae:5.06539\n",
      "[58]\tTest-mae:5.0375\n",
      "[59]\tTest-mae:5.0104\n",
      "[60]\tTest-mae:4.9843\n",
      "[61]\tTest-mae:4.96016\n",
      "[62]\tTest-mae:4.93316\n",
      "[63]\tTest-mae:4.90694\n",
      "[64]\tTest-mae:4.8803\n",
      "[65]\tTest-mae:4.85281\n",
      "[66]\tTest-mae:4.82797\n",
      "[67]\tTest-mae:4.80409\n",
      "[68]\tTest-mae:4.78115\n",
      "[69]\tTest-mae:4.75852\n",
      "[70]\tTest-mae:4.7384\n",
      "[71]\tTest-mae:4.71667\n",
      "[72]\tTest-mae:4.69386\n",
      "[73]\tTest-mae:4.67284\n",
      "[74]\tTest-mae:4.65211\n",
      "[75]\tTest-mae:4.63167\n",
      "[76]\tTest-mae:4.61204\n",
      "[77]\tTest-mae:4.5945\n",
      "[78]\tTest-mae:4.57809\n",
      "[79]\tTest-mae:4.56094\n",
      "[80]\tTest-mae:4.54353\n",
      "[81]\tTest-mae:4.52555\n",
      "[82]\tTest-mae:4.50861\n",
      "[83]\tTest-mae:4.49472\n",
      "[84]\tTest-mae:4.47965\n",
      "[85]\tTest-mae:4.46381\n",
      "[86]\tTest-mae:4.4481\n",
      "[87]\tTest-mae:4.43466\n",
      "[88]\tTest-mae:4.41948\n",
      "[89]\tTest-mae:4.4054\n",
      "[90]\tTest-mae:4.3902\n",
      "[91]\tTest-mae:4.3762\n",
      "[92]\tTest-mae:4.3609\n",
      "[93]\tTest-mae:4.34803\n",
      "[94]\tTest-mae:4.33517\n",
      "[95]\tTest-mae:4.32287\n",
      "[96]\tTest-mae:4.30998\n",
      "[97]\tTest-mae:4.29522\n",
      "[98]\tTest-mae:4.28121\n",
      "[99]\tTest-mae:4.26875\n",
      "[100]\tTest-mae:4.25473\n",
      "[101]\tTest-mae:4.24275\n",
      "[102]\tTest-mae:4.23348\n",
      "[103]\tTest-mae:4.22276\n",
      "[104]\tTest-mae:4.21309\n",
      "[105]\tTest-mae:4.202\n",
      "[106]\tTest-mae:4.19206\n",
      "[107]\tTest-mae:4.18078\n",
      "[108]\tTest-mae:4.17019\n",
      "[109]\tTest-mae:4.16014\n",
      "[110]\tTest-mae:4.15023\n",
      "[111]\tTest-mae:4.14091\n",
      "[112]\tTest-mae:4.13317\n",
      "[113]\tTest-mae:4.12463\n",
      "[114]\tTest-mae:4.11366\n",
      "[115]\tTest-mae:4.10475\n",
      "[116]\tTest-mae:4.09694\n",
      "[117]\tTest-mae:4.09148\n",
      "[118]\tTest-mae:4.0843\n",
      "[119]\tTest-mae:4.07714\n",
      "[120]\tTest-mae:4.06831\n",
      "[121]\tTest-mae:4.06226\n",
      "[122]\tTest-mae:4.05606\n",
      "[123]\tTest-mae:4.05336\n",
      "[124]\tTest-mae:4.05076\n",
      "[125]\tTest-mae:4.04404\n",
      "[126]\tTest-mae:4.03707\n",
      "[127]\tTest-mae:4.03017\n",
      "[128]\tTest-mae:4.02547\n",
      "[129]\tTest-mae:4.02114\n",
      "[130]\tTest-mae:4.01823\n",
      "[131]\tTest-mae:4.01236\n",
      "[132]\tTest-mae:4.00853\n",
      "[133]\tTest-mae:4.00328\n",
      "[134]\tTest-mae:4.00031\n",
      "[135]\tTest-mae:3.99619\n",
      "[136]\tTest-mae:3.9911\n",
      "[137]\tTest-mae:3.99018\n",
      "[138]\tTest-mae:3.98681\n",
      "[139]\tTest-mae:3.98236\n",
      "[140]\tTest-mae:3.98202\n",
      "[141]\tTest-mae:3.97716\n",
      "[142]\tTest-mae:3.97403\n",
      "[143]\tTest-mae:3.97136\n",
      "[144]\tTest-mae:3.96679\n",
      "[145]\tTest-mae:3.96538\n",
      "[146]\tTest-mae:3.9636\n",
      "[147]\tTest-mae:3.96067\n",
      "[148]\tTest-mae:3.95851\n",
      "[149]\tTest-mae:3.95536\n",
      "[150]\tTest-mae:3.95254\n",
      "[151]\tTest-mae:3.95208\n",
      "[152]\tTest-mae:3.9525\n",
      "[153]\tTest-mae:3.95033\n",
      "[154]\tTest-mae:3.94963\n",
      "[155]\tTest-mae:3.94717\n",
      "[156]\tTest-mae:3.94382\n",
      "[157]\tTest-mae:3.94166\n",
      "[158]\tTest-mae:3.93969\n",
      "[159]\tTest-mae:3.93756\n",
      "[160]\tTest-mae:3.93566\n",
      "[161]\tTest-mae:3.93433\n",
      "[162]\tTest-mae:3.9305\n",
      "[163]\tTest-mae:3.92939\n",
      "[164]\tTest-mae:3.92822\n",
      "[165]\tTest-mae:3.92532\n",
      "[166]\tTest-mae:3.92422\n",
      "[167]\tTest-mae:3.92377\n",
      "[168]\tTest-mae:3.92337\n",
      "[169]\tTest-mae:3.92273\n",
      "[170]\tTest-mae:3.92114\n",
      "[171]\tTest-mae:3.92105\n",
      "[172]\tTest-mae:3.91842\n",
      "[173]\tTest-mae:3.91753\n",
      "[174]\tTest-mae:3.91874\n",
      "[175]\tTest-mae:3.91951\n",
      "[176]\tTest-mae:3.91841\n",
      "[177]\tTest-mae:3.91671\n",
      "[178]\tTest-mae:3.9173\n",
      "[179]\tTest-mae:3.91668\n",
      "[180]\tTest-mae:3.9161\n",
      "[181]\tTest-mae:3.91454\n",
      "[182]\tTest-mae:3.91396\n",
      "[183]\tTest-mae:3.91227\n",
      "[184]\tTest-mae:3.91142\n",
      "[185]\tTest-mae:3.91195\n",
      "[186]\tTest-mae:3.91211\n",
      "[187]\tTest-mae:3.9117\n",
      "[188]\tTest-mae:3.91089\n",
      "[189]\tTest-mae:3.90822\n",
      "[190]\tTest-mae:3.90832\n",
      "[191]\tTest-mae:3.90763\n",
      "[192]\tTest-mae:3.90793\n",
      "[193]\tTest-mae:3.90941\n",
      "[194]\tTest-mae:3.91005\n",
      "[195]\tTest-mae:3.90899\n",
      "[196]\tTest-mae:3.91035\n",
      "[197]\tTest-mae:3.90799\n",
      "[198]\tTest-mae:3.90611\n"
     ]
    }
   ],
   "source": [
    "best_model = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(dtest, \"Test\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good, now let's use our model to make predictions. We will use the test dataset and compute MAE with the scikit-learn function. We should obtain the same score as promised in the last round of training, let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.906106619465919"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(best_model.predict(dtest), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! If you want to re-use your model on new data in the future, it can be a good idea to save it to a file, here is how you can do it with xgboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_model(\"my_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then load the model later with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = xgb.Booster()\n",
    "loaded_model.load_model(\"my_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.95622   , 0.33811998, 1.9259537 , ..., 3.821045  , 0.1168783 ,\n",
       "       3.468561  ], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And use it for predictions.\n",
    "loaded_model.predict(dtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
